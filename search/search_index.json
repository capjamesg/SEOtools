{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SEOtools \ud83d\udee0\ufe0f","text":"<p>A set of utilities for SEOs and web developers with which to complete common tasks.</p> <p>With SEOtools, you can:</p> <ol> <li>Programatically add links to related posts in content.</li> <li>Calculate PageRank on internal links from your sitemap.</li> <li>Identify broken links on a web page.</li> <li>Recommend a post to use as canonical for a given keyword.</li> <li>Find the distance of pages from your home page.</li> </ol> <p>And more!</p>"},{"location":"#installation","title":"Installation","text":"<p>You can install SEOtools using pip:</p> <pre><code>pip install seotools\n</code></pre>"},{"location":"#quickstart","title":"Quickstart","text":""},{"location":"#create-a-link-graph","title":"Create a link graph","text":"<pre><code>from seotools.app import Analyzer\n\nanalyzer = Analyzer(\"https://jamesg.blog/sitemap.xml\")\n\nanalyzer.create_link_graph(10, 20)\nanalyzer.compute_pagerank()\nanalyzer.embed_headings()\n</code></pre>"},{"location":"#add-relevant-internal-links-to-a-web-page","title":"Add relevant internal links to a web page","text":"<pre><code>import markdown\n\narticle = markdown.markdown(BeautifulSoup(article.text, \"html.parser\").get_text())\n\nkeyword_replace_count = 0\n\nfor keyword, url in keyword_map.items():\n    if keyword_replace_count &gt;= MAX_KEYWORD_REPLACE:\n        break\n\n    article = article.replace(keyword, f\"&lt;a href='{url}'&gt;{keyword}&lt;/a&gt;\", 1)\n    keyword_replace_count += 1\n\nprint(article)\n</code></pre>"},{"location":"#recommend-related-content-for-a-see-also-section","title":"Recommend related content for a \"See Also\" section","text":"<pre><code>article = requests.get(\"https://jamesg.blog/...\")\n\narticle = markdown.markdown(BeautifulSoup(article.text, \"html.parser\").get_text())\n\nurls = analyzer.recommend_related_content(article.text)\n</code></pre>"},{"location":"#check-if-a-page-contains-a-particular-json-ld-object","title":"Check if a page contains a particular JSON-LD object","text":"<pre><code>from seotools import page_contains_jsonld\nimport requests\n\ncontent = requests.get(\"https://jamesg.blog\")\n\nprint(page_contains_jsonld(content, \"FAQPage\"))\n</code></pre>"},{"location":"#see-also","title":"See Also","text":"<ul> <li>getsitemap: Retrieve URLs in a sitemap. (Web interface)</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under an MIT license.</p>"},{"location":"broken_links/","title":"Identify Broken Links","text":"<p>You can identify broken links using the <code>find_broken_urls()</code> function.</p> <pre><code>from seotools import find_broken_urls\n\nurls_to_check = [\n    \"https://jamesg.blog/\",\n    \"https://jamesg.blog/test/\n]\n\nfor url in urls_to_check:\n    broken_urls = find_broken_urls(url)\n    print(\"Broken URLs identified on \" + url + \":\")\n    print(broken_urls)\n</code></pre>"},{"location":"jsonld/","title":"Identify JSON-LD","text":"<p>You can use the <code>page_contains_jsonld()</code> function to identify if a page contains a specific JSON-LD type (i.e. <code>FAQPage</code> or <code>Article</code>). This is useful for validating the presence of structured data on pages across a website.</p> <p><code>page_contains_jsonld()</code> does not validate the JSON-LD schema. It only checks if a page contains a JSON-LD object of a particular type.</p> <pre><code>from seotools import page_contains_jsonld\nimport requests\n\npage_content = requests.get(\"https://jamesg.blog\").text\n\nprint(page_contains_jsonld(page_content, \"FAQPage\"))\n</code></pre>"},{"location":"recommendation/","title":"Content Recommendations","text":"<p>You can use SEOtools for content recommendation.</p> <p>There are two methods of recommending content:</p> <ol> <li>Automatically adding relevant links to a web page.</li> <li>Identifying content for a \"See Also\" (aka \"Related Posts\") section.</li> </ol> <p>This guide uses the Analyzer() class. Read the fullAnalyzer() documentation for more information.</p>"},{"location":"recommendation/#add-relevant-links-to-a-web-page","title":"Add relevant links to a web page","text":"<pre><code>import markdown\n\narticle = markdown.markdown(BeautifulSoup(article.text, \"html.parser\").get_text())\n\nkeyword_replace_count = 0\n\nfor keyword, url in keyword_map.items():\n    if keyword_replace_count &gt;= MAX_KEYWORD_REPLACE:\n        break\n\n    article = article.replace(keyword, f\"&lt;a href='{url}'&gt;{keyword}&lt;/a&gt;\", 1)\n    keyword_replace_count += 1\n\nprint(article)\n</code></pre>"},{"location":"recommendation/#recommend-related-content-for-a-see-also-section","title":"Recommend related content for a \"See Also\" section","text":"<pre><code>article = requests.get(\"https://jamesg.blog/...\")\n\narticle = markdown.markdown(BeautifulSoup(article.text, \"html.parser\").get_text())\n\nurls = analyzer.recommend_related_content(article.text)\n</code></pre>"},{"location":"reference/analyzer/","title":"Analyzer","text":"<p>An object for analyzing the link structure on a web site, using links from a sitemap.</p> Source code in <code>seotools/app.py</code> <pre><code>class Analyzer:\n    def __init__(self, url):\n        self.sitemap_url = url\n        self.domain = urlparse(url).netloc\n        self.model = None\n        self.reverse_link_graph = None\n\n    def get_subpaths(self) -&gt; list:\n\"\"\"\n        Get all subpaths on a site.\n\n        :return: A list of subpaths.\n        :rtype: list\n        \"\"\"\n        subpaths = {}\n\n        for url in self.link_graph.nodes:\n            url = url.replace(url.strip(\"/\").split(\"/\")[-1], \"\").strip(\"/\")\n            subpaths[url] = subpaths.get(url, []) + [url]\n\n        return subpaths\n\n    def create_link_graph(self, max_workers=20, url_limit=None) -&gt; None:\n\"\"\"\n        Create a link graph of all internal links on a site.\n\n        :param max_workers: The maximum number of threads to use.\n        :type max_workers: int\n        :param url_limit: The maximum number of URLs to process.\n        :type url_limit: int\n\n        :return: None\n        :rtype: None\n        \"\"\"\n\n        sitemap_urls = getsitemap.get_individual_sitemap(self.sitemap_url)\n\n        print(f\"Found {len(sitemap_urls)} sitemaps\")\n\n        # strip / from end of all URLs\n        for key, value in sitemap_urls.items():\n            sitemap_urls[key] = [url.strip(\"/\") for url in value]\n\n        reverse_link_graph = nx.DiGraph()\n\n        self.reverse_link_graph = reverse_link_graph\n\n        internal_link_count = {}\n        heading_information = {}\n\n        # get pagerank\n        G = nx.DiGraph()\n\n        for url in sitemap_urls[self.sitemap_url]:\n            G.add_node(url)\n\n        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n            if url_limit:\n                urls = sitemap_urls[self.sitemap_url][:url_limit]  # [:3]\n            else:\n                urls = sitemap_urls[self.sitemap_url]\n\n            processes = [executor.submit(get_page_urls, url) for url in urls]\n\n            for process in concurrent.futures.as_completed(processes):\n                try:\n                    result = process.result()\n\n                    if not result:\n                        continue\n\n                    links, url, headings = result\n                    heading_information[url] = headings\n\n                    for link in links:\n                        # track all internal links\n                        # canonicalize link\n                        link[\"href\"] = indieweb_utils.canonicalize_url(\n                            link[\"href\"], self.domain, \"https\"\n                        )\n\n                        # must start with https\n                        if not link[\"href\"].startswith(\"https\"):\n                            continue\n\n                        link[\"href\"] = link[\"href\"].split(\"#\")[0]\n                        link[\"href\"] = link[\"href\"].split(\"?\")[0]\n                        link[\"href\"] = link[\"href\"].strip(\"/\")\n\n                        if (\n                            self.domain in link[\"href\"]\n                            and link[\"href\"] != url\n                            and link[\"href\"]\n                            not in internal_link_count.get(link[\"href\"], [])\n                        ):\n                            internal_link_count[link[\"href\"]] = internal_link_count.get(\n                                link[\"href\"], []\n                            ) + [url]\n                            G.add_node(link[\"href\"])\n                            G.add_edge(url, link[\"href\"])\n\n                            # reverse_link_graph.add_node(url)\n                            # reverse_link_graph.add_node(link[\"href\"])\n\n                            # reverse_link_graph.add_edge(link[\"href\"], url)\n\n                except Exception as e:\n                    raise e\n\n        self.heading_information = heading_information\n\n        # dedupe all internal links\n        for key, value in internal_link_count.items():\n            internal_link_count[key] = list(set(value))\n\n        self.link_graph = G\n        self.internal_link_count = internal_link_count\n\n        self.max_page_count = max(\n            [len(value) for value in internal_link_count.values()]\n        )\n\n        # calculate distance between homepage and https://jamesg.blog/category/coffee\n\n        # distance = nx.shortest_path_length(\n        #     self.link_graph, \"https://\" + self.domain, \"https://\" + self.domain + \"/category/coffee\"\n        # )\n        # https://jamesg.blog/category/ -&gt; https://jamesg.blog/category/coffee\n        # print(f\"Distance: {distance}\")\n\n    def remove_most_common_links(self):\n        # used for removing navigation and footer links\n        # remove all links that are on 90% of pages\n        result = {}\n\n        for key, value in self.internal_link_count.items():\n            if len(value) &lt; 0.9 * self.max_page_count:\n                result[key] = value\n\n        return result\n\n    def compute_pagerank(self) -&gt; dict:\n\"\"\"\n        Compute the pagerank of each page in the link graph.\n\n        :return: A dictionary of URLs and their pagerank.\n        :rtype: dict\n        \"\"\"\n        pagerank = nx.pagerank(self.link_graph)\n\n        # order by pagerank in desc\n        sorted_pagerank = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)\n        self.pagerank = pagerank\n\n        return sorted_pagerank\n\n    def save(self) -&gt; None:\n\"\"\"\n        Save the results of an analysis to disk.\n        \"\"\"\n\n        if self.pagerank:\n            with open(\"pagerank.json\", \"w\") as f:\n                json.dump(self.pagerank, f, indent=2)\n\n        if self.link_graph:\n            with open(\"link_graph.json\", \"w\") as f:\n                # save as json\n                link_graph_as_json = nx.node_link_data(self.link_graph)\n\n                json.dump(link_graph_as_json, f, indent=2)\n\n        # save counts\n        if self.internal_link_count:\n            with open(\"internal_link_count.json\", \"w\") as f:\n                json.dump(self.internal_link_count, f, indent=2)\n\n        # save headings\n        if self.heading_information:\n            with open(\"heading_information.json\", \"w\") as f:\n                json.dump(self.heading_information, f, indent=2)\n\n    def _get_distance_from_homepage(self, url: str) -&gt; int:\n\"\"\"\n        Get the distance from the homepage of a URL.\n\n        :param url: The URL to check.\n        :type url: str\n\n        :return: The distance from the homepage.\n        :rtype: int\n        \"\"\"\n        if not self.link_graph or url not in self.link_graph.nodes:\n            return -1\n\n        # crawl from homepage to url\n\n        # visualize reverse link graph\n\n        # from matplotlib import pyplot as plt\n\n        # nx.draw(self.reverse_link_graph, with_labels=True)\n        # plt.show()\n\n        # get all paths from homepage to url\n        if \"https://\" + self.domain not in self.link_graph.nodes:\n            return -1\n\n        try:\n            return nx.shortest_path_length(\n                self.link_graph, \"https://\" + self.domain, url\n            )\n        except nx.NetworkXNoPath:\n            print(f\"No path from homepage to {url}\")\n            return -1\n\n    def get_distances_from_homepage(self) -&gt; dict:\n\"\"\"\n        Get the distance from the homepage of all URLs.\n\n        :return: A dictionary of URLs and their distance from the homepage.\n        :rtype: dict\n        \"\"\"\n        distances = {}\n\n        for url in self.link_graph.nodes:\n            distance = self._get_distance_from_homepage(url)\n\n            if distance == -1:\n                continue\n\n            distances[url] = distance\n\n        return distances\n\n    def embed_headings(self) -&gt; None:\n\"\"\"\n        Create embeddings for all headings on a site.\n\n        :return: None\n        :rtype: None\n        \"\"\"\n        model = sentence_transformers.SentenceTransformer(\n            \"paraphrase-distilroberta-base-v1\"\n        )\n\n        heading_embeddings = {}\n\n        for url, headings in self.heading_information.items():\n            concatenated_headings = \" \".join(headings)\n            heading_embeddings[url] = model.encode(concatenated_headings)\n\n        self.heading_embeddings = heading_embeddings\n\n    def find_most_similar_post_to_query(self, query: str) -&gt; None:\n\"\"\"\n        Find the most similar post to a query.\n\n        :param query: The query to use.\n        :type query: str\n\n        :return: None\n        :rtype: None\n        \"\"\"\n        if not self.model:\n            self.model = self._load_model()\n\n        query_embedding = self.model.encode(query)\n\n        similarities = {}\n\n        for url, embedding in self.heading_embeddings.items():\n            similarities[url] = cosine_similarity([query_embedding], [embedding])[0][0]\n\n        serialized_similarities = {k: v.tolist() for k, v in similarities.items()}\n\n        # zip similarities with PR\n        sorted_similarities_with_pr = {}\n\n        for url, similarity in serialized_similarities.items():\n            sorted_similarities_with_pr[url] = {\n                \"similarity\": similarity,\n                \"pagerank\": self.pagerank[url],\n            }\n\n        # return top 10 results\n        last_heading_similarity = sorted(\n            sorted_similarities_with_pr.items(),\n            key=lambda x: x[1][\"similarity\"],\n            reverse=True,\n        )[:10]\n\n        self.last_heading_similarity = last_heading_similarity\n\n    def is_canonical_best_linked(self, canonical: str) -&gt; bool:\n\"\"\"\n        Check if the canonical URL is the best linked to page.\n\n        :param canonical: The canonical URL.\n        :type canonical: str\n\n        :return: Whether or not the canonical URL is the best linked to page.\n        :rtype: bool\n        \"\"\"\n\n        if canonical not in self.last_heading_similarity:\n            return False\n\n        return self.last_heading_similarity.index(canonical) == 0\n\n    def _load_model(self) -&gt; sentence_transformers.SentenceTransformer:\n        return sentence_transformers.SentenceTransformer(\n            \"paraphrase-distilroberta-base-v1\"\n        )\n\n    def _recommend(self, query: str) -&gt; str:\n\"\"\"\n        Recommend a canonical URL for use with internal link optimization.\n\n        :param query: The query to use.\n        :type query: str\n\n        :return: The canonical URL.\n        :rtype: str\n        \"\"\"\n        if not self.model:\n            self.model = self._load_model()\n\n        query_embedding = self.model.encode(query)\n\n        similarities = {}\n\n        for url, embedding in self.heading_embeddings.items():\n            similarities[url] = cosine_similarity([query_embedding], [embedding])[0][0]\n\n        sorted_similarities = sorted(\n            similarities.items(), key=lambda x: x[1], reverse=True\n        )[:10]\n\n        return sorted_similarities\n\n    def recommend_canonical(self, query):\n        return self._recommend(query)[0][0]\n\n    def recommend_related_content(self, query, allowed_directories=None):\n        allowed_directories = [i.lstrip(\"/\") for i in allowed_directories]\n\n        results = [url for url, _ in self._recommend(query)]\n\n        if allowed_directories:\n            results = [\n                url\n                for url in results\n                if re.match(\n                    f\"https://{self.domain}/({'|'.join(allowed_directories)})\",\n                    url,\n                )\n            ]\n\n        return results\n</code></pre>"},{"location":"reference/analyzer/#seotools.app.Analyzer.compute_pagerank","title":"<code>compute_pagerank()</code>","text":"<p>Compute the pagerank of each page in the link graph.</p> <p>:return: A dictionary of URLs and their pagerank. :rtype: dict</p> Source code in <code>seotools/app.py</code> <pre><code>def compute_pagerank(self) -&gt; dict:\n\"\"\"\n    Compute the pagerank of each page in the link graph.\n\n    :return: A dictionary of URLs and their pagerank.\n    :rtype: dict\n    \"\"\"\n    pagerank = nx.pagerank(self.link_graph)\n\n    # order by pagerank in desc\n    sorted_pagerank = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)\n    self.pagerank = pagerank\n\n    return sorted_pagerank\n</code></pre>"},{"location":"reference/analyzer/#seotools.app.Analyzer.create_link_graph","title":"<code>create_link_graph(max_workers=20, url_limit=None)</code>","text":"<p>Create a link graph of all internal links on a site.</p> <p>:param max_workers: The maximum number of threads to use. :type max_workers: int :param url_limit: The maximum number of URLs to process. :type url_limit: int</p> <p>:return: None :rtype: None</p> Source code in <code>seotools/app.py</code> <pre><code>def create_link_graph(self, max_workers=20, url_limit=None) -&gt; None:\n\"\"\"\n    Create a link graph of all internal links on a site.\n\n    :param max_workers: The maximum number of threads to use.\n    :type max_workers: int\n    :param url_limit: The maximum number of URLs to process.\n    :type url_limit: int\n\n    :return: None\n    :rtype: None\n    \"\"\"\n\n    sitemap_urls = getsitemap.get_individual_sitemap(self.sitemap_url)\n\n    print(f\"Found {len(sitemap_urls)} sitemaps\")\n\n    # strip / from end of all URLs\n    for key, value in sitemap_urls.items():\n        sitemap_urls[key] = [url.strip(\"/\") for url in value]\n\n    reverse_link_graph = nx.DiGraph()\n\n    self.reverse_link_graph = reverse_link_graph\n\n    internal_link_count = {}\n    heading_information = {}\n\n    # get pagerank\n    G = nx.DiGraph()\n\n    for url in sitemap_urls[self.sitemap_url]:\n        G.add_node(url)\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n        if url_limit:\n            urls = sitemap_urls[self.sitemap_url][:url_limit]  # [:3]\n        else:\n            urls = sitemap_urls[self.sitemap_url]\n\n        processes = [executor.submit(get_page_urls, url) for url in urls]\n\n        for process in concurrent.futures.as_completed(processes):\n            try:\n                result = process.result()\n\n                if not result:\n                    continue\n\n                links, url, headings = result\n                heading_information[url] = headings\n\n                for link in links:\n                    # track all internal links\n                    # canonicalize link\n                    link[\"href\"] = indieweb_utils.canonicalize_url(\n                        link[\"href\"], self.domain, \"https\"\n                    )\n\n                    # must start with https\n                    if not link[\"href\"].startswith(\"https\"):\n                        continue\n\n                    link[\"href\"] = link[\"href\"].split(\"#\")[0]\n                    link[\"href\"] = link[\"href\"].split(\"?\")[0]\n                    link[\"href\"] = link[\"href\"].strip(\"/\")\n\n                    if (\n                        self.domain in link[\"href\"]\n                        and link[\"href\"] != url\n                        and link[\"href\"]\n                        not in internal_link_count.get(link[\"href\"], [])\n                    ):\n                        internal_link_count[link[\"href\"]] = internal_link_count.get(\n                            link[\"href\"], []\n                        ) + [url]\n                        G.add_node(link[\"href\"])\n                        G.add_edge(url, link[\"href\"])\n\n                        # reverse_link_graph.add_node(url)\n                        # reverse_link_graph.add_node(link[\"href\"])\n\n                        # reverse_link_graph.add_edge(link[\"href\"], url)\n\n            except Exception as e:\n                raise e\n\n    self.heading_information = heading_information\n\n    # dedupe all internal links\n    for key, value in internal_link_count.items():\n        internal_link_count[key] = list(set(value))\n\n    self.link_graph = G\n    self.internal_link_count = internal_link_count\n\n    self.max_page_count = max(\n        [len(value) for value in internal_link_count.values()]\n    )\n</code></pre>"},{"location":"reference/analyzer/#seotools.app.Analyzer.embed_headings","title":"<code>embed_headings()</code>","text":"<p>Create embeddings for all headings on a site.</p> <p>:return: None :rtype: None</p> Source code in <code>seotools/app.py</code> <pre><code>def embed_headings(self) -&gt; None:\n\"\"\"\n    Create embeddings for all headings on a site.\n\n    :return: None\n    :rtype: None\n    \"\"\"\n    model = sentence_transformers.SentenceTransformer(\n        \"paraphrase-distilroberta-base-v1\"\n    )\n\n    heading_embeddings = {}\n\n    for url, headings in self.heading_information.items():\n        concatenated_headings = \" \".join(headings)\n        heading_embeddings[url] = model.encode(concatenated_headings)\n\n    self.heading_embeddings = heading_embeddings\n</code></pre>"},{"location":"reference/analyzer/#seotools.app.Analyzer.find_most_similar_post_to_query","title":"<code>find_most_similar_post_to_query(query)</code>","text":"<p>Find the most similar post to a query.</p> <p>:param query: The query to use. :type query: str</p> <p>:return: None :rtype: None</p> Source code in <code>seotools/app.py</code> <pre><code>def find_most_similar_post_to_query(self, query: str) -&gt; None:\n\"\"\"\n    Find the most similar post to a query.\n\n    :param query: The query to use.\n    :type query: str\n\n    :return: None\n    :rtype: None\n    \"\"\"\n    if not self.model:\n        self.model = self._load_model()\n\n    query_embedding = self.model.encode(query)\n\n    similarities = {}\n\n    for url, embedding in self.heading_embeddings.items():\n        similarities[url] = cosine_similarity([query_embedding], [embedding])[0][0]\n\n    serialized_similarities = {k: v.tolist() for k, v in similarities.items()}\n\n    # zip similarities with PR\n    sorted_similarities_with_pr = {}\n\n    for url, similarity in serialized_similarities.items():\n        sorted_similarities_with_pr[url] = {\n            \"similarity\": similarity,\n            \"pagerank\": self.pagerank[url],\n        }\n\n    # return top 10 results\n    last_heading_similarity = sorted(\n        sorted_similarities_with_pr.items(),\n        key=lambda x: x[1][\"similarity\"],\n        reverse=True,\n    )[:10]\n\n    self.last_heading_similarity = last_heading_similarity\n</code></pre>"},{"location":"reference/analyzer/#seotools.app.Analyzer.get_distances_from_homepage","title":"<code>get_distances_from_homepage()</code>","text":"<p>Get the distance from the homepage of all URLs.</p> <p>:return: A dictionary of URLs and their distance from the homepage. :rtype: dict</p> Source code in <code>seotools/app.py</code> <pre><code>def get_distances_from_homepage(self) -&gt; dict:\n\"\"\"\n    Get the distance from the homepage of all URLs.\n\n    :return: A dictionary of URLs and their distance from the homepage.\n    :rtype: dict\n    \"\"\"\n    distances = {}\n\n    for url in self.link_graph.nodes:\n        distance = self._get_distance_from_homepage(url)\n\n        if distance == -1:\n            continue\n\n        distances[url] = distance\n\n    return distances\n</code></pre>"},{"location":"reference/analyzer/#seotools.app.Analyzer.get_subpaths","title":"<code>get_subpaths()</code>","text":"<p>Get all subpaths on a site.</p> <p>:return: A list of subpaths. :rtype: list</p> Source code in <code>seotools/app.py</code> <pre><code>def get_subpaths(self) -&gt; list:\n\"\"\"\n    Get all subpaths on a site.\n\n    :return: A list of subpaths.\n    :rtype: list\n    \"\"\"\n    subpaths = {}\n\n    for url in self.link_graph.nodes:\n        url = url.replace(url.strip(\"/\").split(\"/\")[-1], \"\").strip(\"/\")\n        subpaths[url] = subpaths.get(url, []) + [url]\n\n    return subpaths\n</code></pre>"},{"location":"reference/analyzer/#seotools.app.Analyzer.is_canonical_best_linked","title":"<code>is_canonical_best_linked(canonical)</code>","text":"<p>Check if the canonical URL is the best linked to page.</p> <p>:param canonical: The canonical URL. :type canonical: str</p> <p>:return: Whether or not the canonical URL is the best linked to page. :rtype: bool</p> Source code in <code>seotools/app.py</code> <pre><code>def is_canonical_best_linked(self, canonical: str) -&gt; bool:\n\"\"\"\n    Check if the canonical URL is the best linked to page.\n\n    :param canonical: The canonical URL.\n    :type canonical: str\n\n    :return: Whether or not the canonical URL is the best linked to page.\n    :rtype: bool\n    \"\"\"\n\n    if canonical not in self.last_heading_similarity:\n        return False\n\n    return self.last_heading_similarity.index(canonical) == 0\n</code></pre>"},{"location":"reference/analyzer/#seotools.app.Analyzer.save","title":"<code>save()</code>","text":"<p>Save the results of an analysis to disk.</p> Source code in <code>seotools/app.py</code> <pre><code>def save(self) -&gt; None:\n\"\"\"\n    Save the results of an analysis to disk.\n    \"\"\"\n\n    if self.pagerank:\n        with open(\"pagerank.json\", \"w\") as f:\n            json.dump(self.pagerank, f, indent=2)\n\n    if self.link_graph:\n        with open(\"link_graph.json\", \"w\") as f:\n            # save as json\n            link_graph_as_json = nx.node_link_data(self.link_graph)\n\n            json.dump(link_graph_as_json, f, indent=2)\n\n    # save counts\n    if self.internal_link_count:\n        with open(\"internal_link_count.json\", \"w\") as f:\n            json.dump(self.internal_link_count, f, indent=2)\n\n    # save headings\n    if self.heading_information:\n        with open(\"heading_information.json\", \"w\") as f:\n            json.dump(self.heading_information, f, indent=2)\n</code></pre>"}]}