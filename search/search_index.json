{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SEOtools \ud83d\udee0\ufe0f","text":"<p>A set of utilities for SEOs and web developers with which to complete common tasks.</p> <p>With SEOtools, you can:</p> <ol> <li>Programatically add links to related posts in content.</li> <li>Calculate PageRank on internal links from your sitemap.</li> <li>Identify broken links on a web page.</li> <li>Recommend a post to use as canonical for a given keyword.</li> <li>Find the distance of pages from your home page.</li> </ol> <p>And more!</p>"},{"location":"#installation","title":"Installation \ud83d\udcbb","text":"<p>You can install SEOtools using pip:</p> <pre><code>pip install seotools\n</code></pre>"},{"location":"#quickstart","title":"Quickstart \ud83d\ude80","text":""},{"location":"#create-a-link-graph","title":"Create a link graph","text":"<pre><code>from seotools.app import Analyzer\n\nanalyzer = Analyzer(\"https://jamesg.blog/sitemap.xml\")\n\nanalyzer.create_link_graph(10, 20)\nanalyzer.compute_pagerank()\nanalyzer.embed_headings()\n</code></pre>"},{"location":"#get-pagerank-of-a-url","title":"Get pagerank of a URL","text":"<pre><code>print(analyzer.pagerank[\"https://jamesg.blog\"])\n</code></pre>"},{"location":"#add-relevant-internal-links-to-a-web-page","title":"Add relevant internal links to a web page","text":"<pre><code>import markdown\n\narticle = markdown.markdown(BeautifulSoup(article.text, \"html.parser\").get_text())\n\nkeyword_replace_count = 0\n\nfor keyword, url in keyword_map.items():\n    if keyword_replace_count &gt;= MAX_KEYWORD_REPLACE:\n        break\n\n    article = article.replace(keyword, f\"&lt;a href='{url}'&gt;{keyword}&lt;/a&gt;\", 1)\n    keyword_replace_count += 1\n\nprint(article)\n</code></pre>"},{"location":"#recommend-related-content-for-a-see-also-section","title":"Recommend related content for a \"See Also\" section","text":"<pre><code>article = requests.get(\"https://jamesg.blog/...\")\n\narticle = markdown.markdown(BeautifulSoup(article.text, \"html.parser\").get_text())\n\nurls = analyzer.recommend_related_content(article.text)\n</code></pre>"},{"location":"#check-if-a-page-contains-a-particular-json-ld-object","title":"Check if a page contains a particular JSON-LD object","text":"<pre><code>from seotools import page_contains_jsonld\nimport requests\n\ncontent = requests.get(\"https://jamesg.blog\")\n\nprint(page_contains_jsonld(content, \"FAQPage\"))\n</code></pre>"},{"location":"#get-subfolders-in-a-sitemap","title":"Get subfolders in a sitemap","text":"<pre><code>analyzer.get_subpaths()\n</code></pre>"},{"location":"#get-distance-of-url-from-home-page","title":"Get distance of URL from home page","text":"<pre><code>analyzer.get_distance_from_home_page(\"https://jamesg.blog/2023/01/01/\")\n</code></pre>"},{"location":"#retrieve-keywords-that-appear-more-than-n-times-on-a-web-page","title":"Retrieve keywords that appear more than N times on a web page","text":"<pre><code>from seotools import get_keywords\nimport requests\nfrom bs4 import BeautifulSoup\n\narticle = requests.get(\"https://jamesg.blog/...\").text\nparsed_article = BeautifulSoup(article, \"html.parser\").get_text()\n\n# get keywords that appear more than 10 times\nkeywords = get_keywords(parsed_article, 10)\n</code></pre>"},{"location":"#see-also","title":"See Also \ud83d\udcda","text":"<ul> <li>getsitemap: Retrieve URLs in a sitemap. (Web interface)</li> </ul>"},{"location":"#license","title":"License \ud83d\udcdd","text":"<p>This project is licensed under an MIT license.</p>"},{"location":"broken_links/","title":"Identify Broken Links","text":"<p>You can identify broken links using the <code>find_broken_urls()</code> function.</p>"},{"location":"broken_links/#find_broken_urls","title":"find_broken_urls","text":"<p>Find broken URLs.</p> <p>Parameters:</p> Name Type Description Default <code>urls</code> <code>list</code> <p>A list of URLs to check.</p> required <code>timeout</code> <code>int</code> <p>The timeout in seconds. Defaults to 5.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of broken URLs.</p> Example <pre><code>from seotools import find_broken_urls\n\nurls_to_check = [\n    \"https://jamesg.blog/\",\n    \"https://jamesg.blog/test/\n]\n\nfor url in urls_to_check:\n    broken_urls = find_broken_urls(url)\n    print(\"Broken URLs identified on \" + url + \":\")\n    print(broken_urls)\n</code></pre> Source code in <code>seotools/links/broken.py</code> <pre><code>def find_broken_urls(urls: list, timeout: int = 5) -&gt; list:\n\"\"\"\n    Find broken URLs.\n\n    Args:\n        urls (list): A list of URLs to check.\n        timeout (int, optional): The timeout in seconds. Defaults to 5.\n\n    Returns:\n        list: A list of broken URLs.\n\n    Example:\n        ```python\n        from seotools import find_broken_urls\n\n        urls_to_check = [\n            \"https://jamesg.blog/\",\n            \"https://jamesg.blog/test/\n        ]\n\n        for url in urls_to_check:\n            broken_urls = find_broken_urls(url)\n            print(\"Broken URLs identified on \" + url + \":\")\n            print(broken_urls)\n        ```\n    \"\"\"\n    broken_urls = []\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n        future_to_url = {\n            executor.submit(requests.get, url, timeout=timeout): url for url in urls\n        }\n        for future in concurrent.futures.as_completed(future_to_url):\n            url = future_to_url[future]\n\n            try:\n                response = future.result()\n                if response.status_code != 200:\n                    broken_urls.append(url)\n\n            except Exception as exc:\n                broken_urls.append(url)\n                print(f\"{url} generated an exception: {exc}\")\n\n    return broken_urls\n</code></pre>"},{"location":"jsonld/","title":"Identify JSON-LD","text":"<p>You can use the <code>page_contains_jsonld()</code> function to identify if a page contains a specific JSON-LD type (i.e. <code>FAQPage</code> or <code>Article</code>). This is useful for validating the presence of structured data on pages across a website.</p> <p><code>page_contains_jsonld()</code> does not validate the JSON-LD schema. It only checks if a page contains a JSON-LD object of a particular type.</p> <pre><code>from seotools import page_contains_jsonld\nimport requests\n\npage_content = requests.get(\"https://jamesg.blog\").text\n\nprint(page_contains_jsonld(page_content, \"FAQPage\"))\n</code></pre>"},{"location":"recommendation/","title":"Content Recommendations","text":"<p>You can use SEOtools for content recommendation.</p> <p>There are two methods of recommending content:</p> <ol> <li>Automatically adding relevant links to a web page.</li> <li>Identifying content for a \"See Also\" (aka \"Related Posts\") section.</li> </ol> <p>This guide uses the Analyzer() class. Read the fullAnalyzer() documentation for more information.</p>"},{"location":"recommendation/#add-relevant-links-to-a-web-page","title":"Add relevant links to a web page","text":"<pre><code>import markdown\n\narticle = markdown.markdown(BeautifulSoup(article.text, \"html.parser\").get_text())\n\nkeyword_replace_count = 0\n\nfor keyword, url in keyword_map.items():\n    if keyword_replace_count &gt;= MAX_KEYWORD_REPLACE:\n        break\n\n    article = article.replace(keyword, f\"&lt;a href='{url}'&gt;{keyword}&lt;/a&gt;\", 1)\n    keyword_replace_count += 1\n\nprint(article)\n</code></pre>"},{"location":"recommendation/#recommend-related-content-for-a-see-also-section","title":"Recommend related content for a \"See Also\" section","text":"<pre><code>article = requests.get(\"https://jamesg.blog/...\")\n\narticle = markdown.markdown(BeautifulSoup(article.text, \"html.parser\").get_text())\n\nurls = analyzer.recommend_related_content(article.text)\n</code></pre>"},{"location":"topic_clusters/","title":"Topic Clustering","text":"<p>You can use the <code>get_topic_clusters</code> function to cluster the titles of all pages in a sitemap. This function can help you find similar groups of content on your site.</p> <p>This function generates a visualization of the clusters like this:</p> <p></p> <p>You can hover over each link in the analysis generated by the <code>get_topic_clusters</code> function to see the URL of the page.</p>"},{"location":"topic_clusters/#get_topic_clusters","title":"get_topic_clusters","text":"<p>Group content into the provided number of clusters.</p> <p>Parameters:</p> Name Type Description Default <code>topics</code> <code>list</code> <p>A list of topics to cluster.</p> required <code>n_clusters</code> <code>int</code> <p>The number of clusters to create.</p> <code>2</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of clusters.</p> Example <pre><code>from seotools.app import Analyzer\n\nanalyzer = Analyzer(\"https://jamesg.blog/sitemap.xml\", load_from_disk=True)\n\nanalyzer.visualize_with_embeddings()\n</code></pre> Source code in <code>seotools/topics.py</code> <pre><code>def get_topic_clusters(topics: list, n_clusters: int = 2) -&gt; dict:\n\"\"\"\n    Group content into the provided number of clusters.\n\n    Args:\n        topics (list): A list of topics to cluster.\n        n_clusters (int): The number of clusters to create.\n\n    Returns:\n        dict: A dictionary of clusters.\n\n    Example:\n        ```python\n        from seotools.app import Analyzer\n\n        analyzer = Analyzer(\"https://jamesg.blog/sitemap.xml\", load_from_disk=True)\n\n        analyzer.visualize_with_embeddings()\n        ```\n    \"\"\"\n    embeddings = {v: model.encode(v) for v in topics}\n\n    X = list(embeddings.values())\n\n    kmeans = cluster.KMeans(n_clusters=n_clusters)\n\n    kmeans.fit(X)\n\n    labels = kmeans.labels_\n\n    clusters = {}\n\n    for i, label in enumerate(labels):\n        if label not in clusters:\n            clusters[label] = []\n\n        clusters[label].append(list(embeddings.keys())[i])\n\n    # transpose keys into str\n    clusters = {str(k): v for k, v in clusters.items()}\n\n    return clusters\n</code></pre>"},{"location":"helpers/validate_googlebot_ips/","title":"Validate Googlebot IPs","text":"<p>Use the <code>is_google_owned_resource()</code> function to verify if a given IP address is owned by Google.</p> <p>Check if the IP address is owned by Google.</p> <p>Parameters:</p> Name Type Description Default <code>ip</code> <code>str</code> <p>The IP address to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the IP address is owned by Google, False otherwise.</p> Source code in <code>seotools/crawl_bot_validation.py</code> <pre><code>def is_google_owned_resource(ip: str) -&gt; bool:\n\"\"\"\n    Check if the IP address is owned by Google.\n\n    Args:\n        ip (str): The IP address to check.\n\n    Returns:\n        bool: True if the IP address is owned by Google, False otherwise.\n    \"\"\"\n    try:\n        hostname = socket.gethostbyaddr(ip)[0]\n        return hostname in GOOGLE_BOT_HOSTS\n    except:\n        return False\n</code></pre> <pre><code>from seotools import is_google_owned_resource\n\nif is_google_owned_resource(\"0.0.0.0\"):\n    print(\"This IP address is owned by Google.\")\nelse:\n    print(\"This IP address is not owned by Google.\")\n</code></pre>"},{"location":"reference/analyzer/","title":"Analyzer","text":"<p>An object for analyzing the link structure on a web site, using links from a sitemap.</p> Source code in <code>seotools/app.py</code> <pre><code>class Analyzer:\n    def __init__(self, url, max_workers=20, url_limit=None, load_from_disk=False):\n        self.sitemap_url = url\n        self.domain = urlparse(url).netloc\n        self.model = None\n        self.link_graph = None\n        self.page_rank = None\n        self.normalized_page_rank = None\n        self.heading_embeddings = None\n        self.titles = {}\n\n        if load_from_disk and os.path.exists(\"pagerank.json\"):\n            self.load()\n            return\n\n        self.create_link_graph(max_workers, url_limit)\n        self.compute_pagerank()\n        self.embed_headings()\n        self.save()\n\n    def get_subpaths(self) -&gt; list:\n\"\"\"\n        Get all subpaths on a site.\n\n        :return: A list of subpaths.\n        :rtype: list\n        \"\"\"\n        subpaths = {}\n\n        for url in self.link_graph.nodes:\n            url = url.replace(url.strip(\"/\").split(\"/\")[-1], \"\").strip(\"/\")\n            subpaths[url] = subpaths.get(url, []) + [url]\n\n        return subpaths\n\n    def create_link_graph(self, max_workers=20, url_limit=None) -&gt; None:\n\"\"\"\n        Create a link graph of all internal links on a site.\n\n        :param max_workers: The maximum number of threads to use.\n        :type max_workers: int\n        :param url_limit: The maximum number of URLs to process.\n        :type url_limit: int\n\n        :return: None\n        :rtype: None\n        \"\"\"\n\n        sitemap_urls = getsitemap.get_individual_sitemap(self.sitemap_url)\n\n        print(f\"Found {len(sitemap_urls)} sitemaps\")\n\n        # strip / from end of all URLs\n        for key, value in sitemap_urls.items():\n            sitemap_urls[key] = []\n            for url in value:\n                path = urlparse(url).path\n                extension = path.split(\".\")[-1]\n                if extension in (\"png\", \"jpg\", \"jpeg\", \"gif\", \"pdf\"):\n                    continue\n\n                sitemap_urls[key].append(url)\n\n        internal_link_count = {}\n        heading_information = {}\n\n        # get pagerank\n        G = nx.DiGraph()\n\n        for url in sitemap_urls[self.sitemap_url]:\n            G.add_node(url)\n\n        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n            if url_limit:\n                urls = sitemap_urls[self.sitemap_url][:url_limit]  # [:3]\n            else:\n                urls = sitemap_urls[self.sitemap_url]\n\n            processes = [executor.submit(get_page_urls, url) for url in urls]\n\n            for process in concurrent.futures.as_completed(processes):\n                try:\n                    result = process.result()\n\n                    if not result:\n                        continue\n\n                    links, url, headings, title = result\n                    heading_information[url] = headings\n                    self.titles[url] = title\n\n                    for link in links:\n                        # track all internal links\n                        # canonicalize link\n                        link[\"href\"] = indieweb_utils.canonicalize_url(\n                            link[\"href\"], self.domain, \"https\"\n                        )\n\n                        # must start with https\n                        if not link[\"href\"].startswith(\"https\"):\n                            continue\n\n                        link[\"href\"] = link[\"href\"].split(\"#\")[0]\n                        link[\"href\"] = link[\"href\"].split(\"?\")[0]\n                        link[\"href\"] = link[\"href\"].strip(\"/\")\n\n                        extension = link[\"href\"].split(\".\")[-1]\n\n                        if extension in [\"jpg\", \"png\", \"gif\", \"jpeg\", \"pdf\"]:\n                            continue\n\n                        if (\n                            self.domain in link[\"href\"]\n                            and link[\"href\"] != url\n                            and link[\"href\"]\n                            not in internal_link_count.get(link[\"href\"], [])\n                        ):\n                            internal_link_count[link[\"href\"]] = internal_link_count.get(\n                                link[\"href\"], []\n                            ) + [url]\n                            G.add_node(link[\"href\"])\n                            G.add_edge(url, link[\"href\"])\n\n                except Exception as e:\n                    raise e\n\n        self.heading_information = heading_information\n\n        # dedupe all internal links\n        for key, value in internal_link_count.items():\n            internal_link_count[key] = list(set(value))\n\n        self.link_graph = G\n        self.internal_link_count = internal_link_count\n\n        self.max_page_count = max(\n            [len(value) for value in internal_link_count.values()]\n        )\n\n    def visualize_with_embeddings(self) -&gt; None:\n        embeddings = self.heading_embeddings.values()\n\n        # convert to list\n        embeddings = list(embeddings)\n\n        # convert to numpy array\n        embeddings = np.array(embeddings)\n\n        # convert to 2d\n        embeddings = TSNE(n_components=2).fit_transform(embeddings)\n\n        # add labels\n        fig = go.Figure(\n            data=go.Scatter(\n                x=embeddings[:, 0],\n                y=embeddings[:, 1],\n                text=list(self.heading_embeddings.keys()),\n                mode=\"markers\",\n                # use cluster color\n                marker=dict(\n                    size=16,\n                    colorscale=\"Viridis\",\n                    showscale=True,\n                ),\n            )\n        )\n\n        fig.show()\n\n    def remove_most_common_links(self):\n        # used for removing navigation and footer links\n        # remove all links that are on 90% of pages\n        result = {}\n\n        for key, value in self.internal_link_count.items():\n            if len(value) &lt; 0.9 * self.max_page_count:\n                result[key] = value\n\n        return result\n\n    def compute_pagerank(self) -&gt; dict:\n\"\"\"\n        Compute the pagerank of each page in the link graph.\n\n        :return: A dictionary of URLs and their pagerank.\n        :rtype: dict\n        \"\"\"\n        pagerank = nx.pagerank(self.link_graph)\n\n        # order by pagerank in desc\n        sorted_pagerank = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)\n        self.page_rank = pagerank\n\n        normalized_pagerank = {}\n\n        # score should be between 0 and 100\n        # 0 is lowest, 100 is highest\n\n        max_pagerank = max([value for _, value in sorted_pagerank])\n        min_pagerank = min([value for _, value in sorted_pagerank])\n\n        for url, score in sorted_pagerank:\n            normalized_pagerank[url] = math.floor(\n                100 * (score - min_pagerank) / (max_pagerank - min_pagerank)\n            )\n\n        print(\"Normalized pagerank\")\n        print(normalized_pagerank)\n\n        # sort by normalized pagerank then save to file\n        sorted_pagerank = sorted(\n            normalized_pagerank.items(), key=lambda x: x[1], reverse=True\n        )\n\n        with open(\"normalized_pagerank.json\", \"w\") as f:\n            json.dump(sorted_pagerank, f, indent=2)\n\n        return sorted_pagerank\n\n    def save(self) -&gt; None:\n\"\"\"\n        Save the results of an analysis to disk.\n        \"\"\"\n\n        if self.page_rank:\n            with open(\"pagerank.json\", \"w\") as f:\n                json.dump(self.page_rank, f, indent=2)\n\n        if self.link_graph:\n            with open(\"link_graph.json\", \"w\") as f:\n                # save as json\n                link_graph_as_json = nx.node_link_data(self.link_graph)\n\n                json.dump(link_graph_as_json, f, indent=2)\n\n        # save counts\n        if self.internal_link_count:\n            with open(\"internal_link_count.json\", \"w\") as f:\n                json.dump(self.internal_link_count, f, indent=2)\n\n        # save headings\n        if self.heading_information:\n            with open(\"heading_information.json\", \"w\") as f:\n                json.dump(self.heading_information, f, indent=2)\n\n        if self.titles:\n            with open(\"titles.json\", \"w\") as f:\n                json.dump(self.titles, f, indent=2)\n\n    def load(self):\n\"\"\"\n        Load the results of an analysis from disk.\n\n        :return: An Analyzer object.\n        :rtype: Analyzer\n        \"\"\"\n\n        with open(\"pagerank.json\", \"r\") as f:\n            self.pagerank = json.load(f)\n\n        with open(\"link_graph.json\", \"r\") as f:\n            link_graph_as_json = json.load(f)\n\n            self.link_graph = nx.node_link_graph(link_graph_as_json)\n\n        with open(\"internal_link_count.json\", \"r\") as f:\n            self.internal_link_count = json.load(f)\n\n        with open(\"heading_information.json\", \"r\") as f:\n            self.heading_information = json.load(f)\n\n        with open(\"titles.json\", \"r\") as f:\n            self.titles = json.load(f)\n\n        print(\n            \"Loaded pagerank, link graph, internal link count and heading information\"\n        )\n        self.embed_headings()\n\n    def _get_distance_from_homepage(self, url: str) -&gt; int:\n\"\"\"\n        Get the distance from the homepage of a URL.\n\n        :param url: The URL to check.\n        :type url: str\n\n        :return: The distance from the homepage.\n        :rtype: int\n        \"\"\"\n        if not self.link_graph or url not in self.link_graph.nodes:\n            return -1\n\n        if \"https://\" + self.domain not in self.link_graph.nodes:\n            return -1\n\n        try:\n            return nx.shortest_path_length(\n                self.link_graph, \"https://\" + self.domain, url\n            )\n        except nx.NetworkXNoPath:\n            print(f\"No path from homepage to {url}\")\n            return -1\n\n    def get_distances_from_homepage(self) -&gt; dict:\n\"\"\"\n        Get the distance from the homepage of all URLs.\n\n        :return: A dictionary of URLs and their distance from the homepage.\n        :rtype: dict\n        \"\"\"\n        distances = {}\n\n        for url in self.link_graph.nodes:\n            distance = self._get_distance_from_homepage(url)\n\n            if distance == -1:\n                continue\n\n            distances[url] = distance\n\n        return distances\n\n    def embed_headings(self) -&gt; None:\n\"\"\"\n        Create embeddings for all headings on a site.\n\n        :return: None\n        :rtype: None\n        \"\"\"\n        model = sentence_transformers.SentenceTransformer(\n            \"paraphrase-distilroberta-base-v1\"\n        )\n\n        heading_embeddings = {}\n\n        for url, headings in self.heading_information.items():\n            concatenated_headings = \" \".join(headings)\n            heading_embeddings[url] = model.encode(concatenated_headings)\n\n        self.heading_embeddings = heading_embeddings\n\n    def find_most_similar_post_to_query(self, query: str) -&gt; None:\n\"\"\"\n        Find the most similar post to a query.\n\n        :param query: The query to use.\n        :type query: str\n\n        :return: None\n        :rtype: None\n        \"\"\"\n        if not self.model:\n            self.model = self._load_model()\n\n        query_embedding = self.model.encode(query)\n\n        similarities = {}\n\n        for url, embedding in self.heading_embeddings.items():\n            similarities[url] = cosine_similarity([query_embedding], [embedding])[0][0]\n\n        serialized_similarities = {k: v.tolist() for k, v in similarities.items()}\n\n        # zip similarities with PR\n        sorted_similarities_with_pr = {}\n\n        for url, similarity in serialized_similarities.items():\n            sorted_similarities_with_pr[url] = {\n                \"similarity\": similarity,\n                \"pagerank\": self.pagerank[url],\n            }\n\n        # return top 10 results\n        last_heading_similarity = sorted(\n            sorted_similarities_with_pr.items(),\n            key=lambda x: x[1][\"similarity\"],\n            reverse=True,\n        )[:10]\n\n        self.last_heading_similarity = last_heading_similarity\n\n    def is_canonical_best_linked(self, canonical: str) -&gt; bool:\n\"\"\"\n        Check if the canonical URL is the best linked to page.\n\n        :param canonical: The canonical URL.\n        :type canonical: str\n\n        :return: Whether or not the canonical URL is the best linked to page.\n        :rtype: bool\n        \"\"\"\n\n        if canonical not in self.last_heading_similarity:\n            return False\n\n        return self.last_heading_similarity.index(canonical) == 0\n\n    def _load_model(self) -&gt; sentence_transformers.SentenceTransformer:\n        return sentence_transformers.SentenceTransformer(\n            \"paraphrase-distilroberta-base-v1\"\n        )\n\n    def find_pages_with_under_n_links(self, n: int) -&gt; list:\n\"\"\"\n        Find all pages with only one link.\n\n        :return: A list of URLs.\n        :rtype: list\n        \"\"\"\n\n        results = [\n            url for url in self.link_graph.nodes if self.link_graph.degree[url] &lt; n\n        ]\n        results = sorted(results)\n\n        return results\n\n    def _recommend(self, query: str) -&gt; str:\n\"\"\"\n        Recommend a canonical URL for use with internal link optimization.\n\n        :param query: The query to use.\n        :type query: str\n\n        :return: The canonical URL.\n        :rtype: str\n        \"\"\"\n        if not self.model:\n            self.model = self._load_model()\n\n        query_embedding = self.model.encode(query)\n\n        similarities = {}\n\n        for url, embedding in self.heading_embeddings.items():\n            similarities[url] = cosine_similarity([query_embedding], [embedding])[0][0]\n\n        sorted_similarities = sorted(\n            similarities.items(), key=lambda x: x[1], reverse=True\n        )[:10]\n\n        return sorted_similarities\n\n    def recommend_canonical(self, query):\n        return self._recommend(query)[0][0]\n\n    def recommend_related_content(self, query, allowed_directories=[]):\n        allowed_directories = [i.lstrip(\"/\") for i in allowed_directories]\n\n        results = [url for url, _ in self._recommend(query)]\n\n        if len(allowed_directories):\n            rule = lambda url: re.match(\n                f\"https://{self.domain}/({'|'.join(allowed_directories)})\",\n                url,\n            )\n        else:\n            rule = lambda url: re.match(f\"https://{self.domain}\", url)\n\n        if allowed_directories:\n            results = [\n                url for url in results if rule(url) and url not in allowed_directories\n            ]\n\n        return results\n</code></pre>"},{"location":"reference/analyzer/#seotools.app.Analyzer.compute_pagerank","title":"<code>compute_pagerank()</code>","text":"<p>Compute the pagerank of each page in the link graph.</p> <p>:return: A dictionary of URLs and their pagerank. :rtype: dict</p> Source code in <code>seotools/app.py</code> <pre><code>def compute_pagerank(self) -&gt; dict:\n\"\"\"\n    Compute the pagerank of each page in the link graph.\n\n    :return: A dictionary of URLs and their pagerank.\n    :rtype: dict\n    \"\"\"\n    pagerank = nx.pagerank(self.link_graph)\n\n    # order by pagerank in desc\n    sorted_pagerank = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)\n    self.page_rank = pagerank\n\n    normalized_pagerank = {}\n\n    # score should be between 0 and 100\n    # 0 is lowest, 100 is highest\n\n    max_pagerank = max([value for _, value in sorted_pagerank])\n    min_pagerank = min([value for _, value in sorted_pagerank])\n\n    for url, score in sorted_pagerank:\n        normalized_pagerank[url] = math.floor(\n            100 * (score - min_pagerank) / (max_pagerank - min_pagerank)\n        )\n\n    print(\"Normalized pagerank\")\n    print(normalized_pagerank)\n\n    # sort by normalized pagerank then save to file\n    sorted_pagerank = sorted(\n        normalized_pagerank.items(), key=lambda x: x[1], reverse=True\n    )\n\n    with open(\"normalized_pagerank.json\", \"w\") as f:\n        json.dump(sorted_pagerank, f, indent=2)\n\n    return sorted_pagerank\n</code></pre>"},{"location":"reference/analyzer/#seotools.app.Analyzer.create_link_graph","title":"<code>create_link_graph(max_workers=20, url_limit=None)</code>","text":"<p>Create a link graph of all internal links on a site.</p> <p>:param max_workers: The maximum number of threads to use. :type max_workers: int :param url_limit: The maximum number of URLs to process. :type url_limit: int</p> <p>:return: None :rtype: None</p> Source code in <code>seotools/app.py</code> <pre><code>def create_link_graph(self, max_workers=20, url_limit=None) -&gt; None:\n\"\"\"\n    Create a link graph of all internal links on a site.\n\n    :param max_workers: The maximum number of threads to use.\n    :type max_workers: int\n    :param url_limit: The maximum number of URLs to process.\n    :type url_limit: int\n\n    :return: None\n    :rtype: None\n    \"\"\"\n\n    sitemap_urls = getsitemap.get_individual_sitemap(self.sitemap_url)\n\n    print(f\"Found {len(sitemap_urls)} sitemaps\")\n\n    # strip / from end of all URLs\n    for key, value in sitemap_urls.items():\n        sitemap_urls[key] = []\n        for url in value:\n            path = urlparse(url).path\n            extension = path.split(\".\")[-1]\n            if extension in (\"png\", \"jpg\", \"jpeg\", \"gif\", \"pdf\"):\n                continue\n\n            sitemap_urls[key].append(url)\n\n    internal_link_count = {}\n    heading_information = {}\n\n    # get pagerank\n    G = nx.DiGraph()\n\n    for url in sitemap_urls[self.sitemap_url]:\n        G.add_node(url)\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n        if url_limit:\n            urls = sitemap_urls[self.sitemap_url][:url_limit]  # [:3]\n        else:\n            urls = sitemap_urls[self.sitemap_url]\n\n        processes = [executor.submit(get_page_urls, url) for url in urls]\n\n        for process in concurrent.futures.as_completed(processes):\n            try:\n                result = process.result()\n\n                if not result:\n                    continue\n\n                links, url, headings, title = result\n                heading_information[url] = headings\n                self.titles[url] = title\n\n                for link in links:\n                    # track all internal links\n                    # canonicalize link\n                    link[\"href\"] = indieweb_utils.canonicalize_url(\n                        link[\"href\"], self.domain, \"https\"\n                    )\n\n                    # must start with https\n                    if not link[\"href\"].startswith(\"https\"):\n                        continue\n\n                    link[\"href\"] = link[\"href\"].split(\"#\")[0]\n                    link[\"href\"] = link[\"href\"].split(\"?\")[0]\n                    link[\"href\"] = link[\"href\"].strip(\"/\")\n\n                    extension = link[\"href\"].split(\".\")[-1]\n\n                    if extension in [\"jpg\", \"png\", \"gif\", \"jpeg\", \"pdf\"]:\n                        continue\n\n                    if (\n                        self.domain in link[\"href\"]\n                        and link[\"href\"] != url\n                        and link[\"href\"]\n                        not in internal_link_count.get(link[\"href\"], [])\n                    ):\n                        internal_link_count[link[\"href\"]] = internal_link_count.get(\n                            link[\"href\"], []\n                        ) + [url]\n                        G.add_node(link[\"href\"])\n                        G.add_edge(url, link[\"href\"])\n\n            except Exception as e:\n                raise e\n\n    self.heading_information = heading_information\n\n    # dedupe all internal links\n    for key, value in internal_link_count.items():\n        internal_link_count[key] = list(set(value))\n\n    self.link_graph = G\n    self.internal_link_count = internal_link_count\n\n    self.max_page_count = max(\n        [len(value) for value in internal_link_count.values()]\n    )\n</code></pre>"},{"location":"reference/analyzer/#seotools.app.Analyzer.embed_headings","title":"<code>embed_headings()</code>","text":"<p>Create embeddings for all headings on a site.</p> <p>:return: None :rtype: None</p> Source code in <code>seotools/app.py</code> <pre><code>def embed_headings(self) -&gt; None:\n\"\"\"\n    Create embeddings for all headings on a site.\n\n    :return: None\n    :rtype: None\n    \"\"\"\n    model = sentence_transformers.SentenceTransformer(\n        \"paraphrase-distilroberta-base-v1\"\n    )\n\n    heading_embeddings = {}\n\n    for url, headings in self.heading_information.items():\n        concatenated_headings = \" \".join(headings)\n        heading_embeddings[url] = model.encode(concatenated_headings)\n\n    self.heading_embeddings = heading_embeddings\n</code></pre>"},{"location":"reference/analyzer/#seotools.app.Analyzer.find_most_similar_post_to_query","title":"<code>find_most_similar_post_to_query(query)</code>","text":"<p>Find the most similar post to a query.</p> <p>:param query: The query to use. :type query: str</p> <p>:return: None :rtype: None</p> Source code in <code>seotools/app.py</code> <pre><code>def find_most_similar_post_to_query(self, query: str) -&gt; None:\n\"\"\"\n    Find the most similar post to a query.\n\n    :param query: The query to use.\n    :type query: str\n\n    :return: None\n    :rtype: None\n    \"\"\"\n    if not self.model:\n        self.model = self._load_model()\n\n    query_embedding = self.model.encode(query)\n\n    similarities = {}\n\n    for url, embedding in self.heading_embeddings.items():\n        similarities[url] = cosine_similarity([query_embedding], [embedding])[0][0]\n\n    serialized_similarities = {k: v.tolist() for k, v in similarities.items()}\n\n    # zip similarities with PR\n    sorted_similarities_with_pr = {}\n\n    for url, similarity in serialized_similarities.items():\n        sorted_similarities_with_pr[url] = {\n            \"similarity\": similarity,\n            \"pagerank\": self.pagerank[url],\n        }\n\n    # return top 10 results\n    last_heading_similarity = sorted(\n        sorted_similarities_with_pr.items(),\n        key=lambda x: x[1][\"similarity\"],\n        reverse=True,\n    )[:10]\n\n    self.last_heading_similarity = last_heading_similarity\n</code></pre>"},{"location":"reference/analyzer/#seotools.app.Analyzer.find_pages_with_under_n_links","title":"<code>find_pages_with_under_n_links(n)</code>","text":"<p>Find all pages with only one link.</p> <p>:return: A list of URLs. :rtype: list</p> Source code in <code>seotools/app.py</code> <pre><code>def find_pages_with_under_n_links(self, n: int) -&gt; list:\n\"\"\"\n    Find all pages with only one link.\n\n    :return: A list of URLs.\n    :rtype: list\n    \"\"\"\n\n    results = [\n        url for url in self.link_graph.nodes if self.link_graph.degree[url] &lt; n\n    ]\n    results = sorted(results)\n\n    return results\n</code></pre>"},{"location":"reference/analyzer/#seotools.app.Analyzer.get_distances_from_homepage","title":"<code>get_distances_from_homepage()</code>","text":"<p>Get the distance from the homepage of all URLs.</p> <p>:return: A dictionary of URLs and their distance from the homepage. :rtype: dict</p> Source code in <code>seotools/app.py</code> <pre><code>def get_distances_from_homepage(self) -&gt; dict:\n\"\"\"\n    Get the distance from the homepage of all URLs.\n\n    :return: A dictionary of URLs and their distance from the homepage.\n    :rtype: dict\n    \"\"\"\n    distances = {}\n\n    for url in self.link_graph.nodes:\n        distance = self._get_distance_from_homepage(url)\n\n        if distance == -1:\n            continue\n\n        distances[url] = distance\n\n    return distances\n</code></pre>"},{"location":"reference/analyzer/#seotools.app.Analyzer.get_subpaths","title":"<code>get_subpaths()</code>","text":"<p>Get all subpaths on a site.</p> <p>:return: A list of subpaths. :rtype: list</p> Source code in <code>seotools/app.py</code> <pre><code>def get_subpaths(self) -&gt; list:\n\"\"\"\n    Get all subpaths on a site.\n\n    :return: A list of subpaths.\n    :rtype: list\n    \"\"\"\n    subpaths = {}\n\n    for url in self.link_graph.nodes:\n        url = url.replace(url.strip(\"/\").split(\"/\")[-1], \"\").strip(\"/\")\n        subpaths[url] = subpaths.get(url, []) + [url]\n\n    return subpaths\n</code></pre>"},{"location":"reference/analyzer/#seotools.app.Analyzer.is_canonical_best_linked","title":"<code>is_canonical_best_linked(canonical)</code>","text":"<p>Check if the canonical URL is the best linked to page.</p> <p>:param canonical: The canonical URL. :type canonical: str</p> <p>:return: Whether or not the canonical URL is the best linked to page. :rtype: bool</p> Source code in <code>seotools/app.py</code> <pre><code>def is_canonical_best_linked(self, canonical: str) -&gt; bool:\n\"\"\"\n    Check if the canonical URL is the best linked to page.\n\n    :param canonical: The canonical URL.\n    :type canonical: str\n\n    :return: Whether or not the canonical URL is the best linked to page.\n    :rtype: bool\n    \"\"\"\n\n    if canonical not in self.last_heading_similarity:\n        return False\n\n    return self.last_heading_similarity.index(canonical) == 0\n</code></pre>"},{"location":"reference/analyzer/#seotools.app.Analyzer.load","title":"<code>load()</code>","text":"<p>Load the results of an analysis from disk.</p> <p>:return: An Analyzer object. :rtype: Analyzer</p> Source code in <code>seotools/app.py</code> <pre><code>def load(self):\n\"\"\"\n    Load the results of an analysis from disk.\n\n    :return: An Analyzer object.\n    :rtype: Analyzer\n    \"\"\"\n\n    with open(\"pagerank.json\", \"r\") as f:\n        self.pagerank = json.load(f)\n\n    with open(\"link_graph.json\", \"r\") as f:\n        link_graph_as_json = json.load(f)\n\n        self.link_graph = nx.node_link_graph(link_graph_as_json)\n\n    with open(\"internal_link_count.json\", \"r\") as f:\n        self.internal_link_count = json.load(f)\n\n    with open(\"heading_information.json\", \"r\") as f:\n        self.heading_information = json.load(f)\n\n    with open(\"titles.json\", \"r\") as f:\n        self.titles = json.load(f)\n\n    print(\n        \"Loaded pagerank, link graph, internal link count and heading information\"\n    )\n    self.embed_headings()\n</code></pre>"},{"location":"reference/analyzer/#seotools.app.Analyzer.save","title":"<code>save()</code>","text":"<p>Save the results of an analysis to disk.</p> Source code in <code>seotools/app.py</code> <pre><code>def save(self) -&gt; None:\n\"\"\"\n    Save the results of an analysis to disk.\n    \"\"\"\n\n    if self.page_rank:\n        with open(\"pagerank.json\", \"w\") as f:\n            json.dump(self.page_rank, f, indent=2)\n\n    if self.link_graph:\n        with open(\"link_graph.json\", \"w\") as f:\n            # save as json\n            link_graph_as_json = nx.node_link_data(self.link_graph)\n\n            json.dump(link_graph_as_json, f, indent=2)\n\n    # save counts\n    if self.internal_link_count:\n        with open(\"internal_link_count.json\", \"w\") as f:\n            json.dump(self.internal_link_count, f, indent=2)\n\n    # save headings\n    if self.heading_information:\n        with open(\"heading_information.json\", \"w\") as f:\n            json.dump(self.heading_information, f, indent=2)\n\n    if self.titles:\n        with open(\"titles.json\", \"w\") as f:\n            json.dump(self.titles, f, indent=2)\n</code></pre>"},{"location":"reference/crawl_log_analyzer/","title":"CrawlLogAnalyzer","text":"<p>Analyze crawl logs to identify trends.</p> Source code in <code>seotools/logs.py</code> <pre><code>class CrawlLogAnalyzer:\n\"\"\"\n    Analyze crawl logs to identify trends.\n    \"\"\"\n    def __init__(self, log_file_name: str, validators: list[str] = []) -&gt; None:\n        self.log_file_name = log_file_name\n        # header row is not present in the log file, but uses\n        # the following format:\n        # regex from https://regex101.com/library/P801k2\n        log_file = pd.read_csv(log_file_name, sep=r'\\s(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)(?![^\\[]*\\])', header=None, usecols=[0, 3, 4, 5, 6, 7, 8], engine='python')\n\n        self.log_file = log_file\n        labels = ['ip', 'time_local', 'request', 'status', 'body_bytes_sent', 'http_referer', 'http_user_agent']\n\n        # rename columns\n        self.log_file.columns = labels\n\n        if \"googlebot\" in validators:\n            print(\"Filtering out all non-Googlebot IPs...\")\n            # unique ips where \"google\" is in the user agent\n            unique_ips = self.log_file[self.log_file['http_user_agent'].str.contains(\"Google\", na=False)]['ip'].unique()\n            # get googlebot ips\n            googlebot_ips = [ip for ip in tqdm.tqdm(unique_ips) if is_google_owned_resource(ip)]\n\n            # filter out googlebot ips\n            self.log_file = self.log_file[~self.log_file['ip'].isin(googlebot_ips)]\n\n\n        # split up request column into path and protocol\n        self.log_file['path'] = self.log_file['request'].apply(lambda x: x.split(' ')[1] if len(x.split(' ')) &gt; 1 else x)\n        self.log_file['protocol'] = self.log_file['request'].apply(lambda x: x.split(' ')[2] if len(x.split(' ')) &gt; 2 else x)\n\n        # drop the original request column\n        self.log_file.drop('request', axis=1, inplace=True)\n\n        # get rid of the first column\n\n    def get_unique(self, col: str) -&gt; list:\n\"\"\"\n        Get the unique values in a column.\n\n        Args:\n            col (str): The column to analyze.\n\n        Returns:\n            list: A list of unique values in the column.\n\n        Example:\n            ```python\n            from seotools.logs import CrawLogAnalyzer\n\n            analyzer = CrawlLogAnalyzer(\"access.log\")\n\n            analyzer.get_unique(\"request\")\n            ```\n        \"\"\"\n        return self.log_file[col].unique()\n\n    def get_count(self, col: str) -&gt; dict:\n\"\"\"\n        Count the number of times a value appears in a column.\n\n        Args:\n            col (str): The column to analyze.\n\n        Returns:\n            dict: A dictionary of values and the number of times they appear in the column.\n        \"\"\"\n        data =  self.log_file[col].value_counts().to_dict()\n\n        return {k: v for k, v in data.items() if k != '-'}\n\n    def crawl_frequency_by_url(self, url: str) -&gt; int:\n\"\"\"\n        Find the number of times a URL has been crawled.\n\n        Args:\n            url (str): The URL to analyze.\n\n        Returns:\n            int: The number of times the URL was crawled.\n        \"\"\"\n\n        return self.log_file[self.log_file[0] == url].shape[0]\n\n    def _get_avg_space_between_crawls(self, crawls_by_date, url):\n        # get average space between crawls\n        dates = list(crawls_by_date.keys())\n        # cast dates to ints 01/Aug/2020\n\n        dates = [datetime.datetime.strptime(date, '%d/%b/%Y') for date in dates]\n\n        # get difference between dates\n\n        diffs = [dates[i] - dates[i - 1] for i in range(1, len(dates))]\n\n        # convert to days\n        diffs = [diff.days for diff in diffs]\n\n        # get average\n        avg_diff = sum(diffs) / len(diffs)\n\n        # get average daily crawls for the url\n        avg_daily_crawls = self.log_file[self.log_file['path'] == url].shape[0] / len(dates)\n\n        return avg_diff, avg_daily_crawls\n\n    def get_top_urls(self, n: int = 10) -&gt; dict:\n\"\"\"\n        Find the top n most crawled URLs.\n\n        Args:\n            n (int): The number of URLs to return.\n\n        Returns:\n            dict: A dictionary of URLs and the number of times they were crawled.\n        \"\"\"\n        return self.log_file['path'].value_counts().head(n).to_dict()\n\n    def crawl_frequency_aggregate(self, url: str = None, path: str = None) -&gt; dict:\n\"\"\"\n        Find the number of times a URL has been crawled by date.\n\n        Args:\n            url (str): The URL to analyze.\n            path (str): The path to analyze.\n\n        Returns:\n            dict: A dictionary of dates and the number of times the URL was crawled on that date.\n\n        Example:\n            ```python\n            from seotools.logs import CrawLogAnalyzer\n\n            analyzer = CrawlLogAnalyzer(\"access.log\")\n\n            analyzer.crawl_frequency_aggregate(url=\"...\")\n            ```\n        \"\"\"\n\n        crawls_by_date = {}\n\n        if path:\n            url = path\n        else:\n            url = url\n\n        if not path and not url:\n            raise Exception(\"You must provide either a path or a URL.\")\n\n        self.log_file['formatted_date'] = self.log_file['time_local'].apply(lambda x: x.split(':')[0].replace('[', ''))\n\n        print(\"Getting crawl frequency...\")\n\n        for date in tqdm.tqdm(self.log_file['formatted_date'].unique()):\n            if url:\n                crawls_by_date[date] = self.log_file[(self.log_file['formatted_date'] == date) &amp; (self.log_file['path'] == url)].shape[0]\n            else:\n                crawls_by_date[date] = self.log_file[self.log_file['formatted_date'] == date].shape[0]\n\n        # order by date\n        crawls_by_date = {k: v for k, v in sorted(crawls_by_date.items(), key=lambda item: item[0])}\n\n        # return how avg. space between crawls\n        avg_diff, avg_daily_crawls = self._get_avg_space_between_crawls(crawls_by_date, url)\n\n        return crawls_by_date, avg_diff, avg_daily_crawls\n</code></pre>"},{"location":"reference/crawl_log_analyzer/#seotools.logs.CrawlLogAnalyzer.crawl_frequency_aggregate","title":"<code>crawl_frequency_aggregate(url=None, path=None)</code>","text":"<p>Find the number of times a URL has been crawled by date.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to analyze.</p> <code>None</code> <code>path</code> <code>str</code> <p>The path to analyze.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of dates and the number of times the URL was crawled on that date.</p> Example <pre><code>from seotools.logs import CrawLogAnalyzer\n\nanalyzer = CrawlLogAnalyzer(\"access.log\")\n\nanalyzer.crawl_frequency_aggregate(url=\"...\")\n</code></pre> Source code in <code>seotools/logs.py</code> <pre><code>def crawl_frequency_aggregate(self, url: str = None, path: str = None) -&gt; dict:\n\"\"\"\n    Find the number of times a URL has been crawled by date.\n\n    Args:\n        url (str): The URL to analyze.\n        path (str): The path to analyze.\n\n    Returns:\n        dict: A dictionary of dates and the number of times the URL was crawled on that date.\n\n    Example:\n        ```python\n        from seotools.logs import CrawLogAnalyzer\n\n        analyzer = CrawlLogAnalyzer(\"access.log\")\n\n        analyzer.crawl_frequency_aggregate(url=\"...\")\n        ```\n    \"\"\"\n\n    crawls_by_date = {}\n\n    if path:\n        url = path\n    else:\n        url = url\n\n    if not path and not url:\n        raise Exception(\"You must provide either a path or a URL.\")\n\n    self.log_file['formatted_date'] = self.log_file['time_local'].apply(lambda x: x.split(':')[0].replace('[', ''))\n\n    print(\"Getting crawl frequency...\")\n\n    for date in tqdm.tqdm(self.log_file['formatted_date'].unique()):\n        if url:\n            crawls_by_date[date] = self.log_file[(self.log_file['formatted_date'] == date) &amp; (self.log_file['path'] == url)].shape[0]\n        else:\n            crawls_by_date[date] = self.log_file[self.log_file['formatted_date'] == date].shape[0]\n\n    # order by date\n    crawls_by_date = {k: v for k, v in sorted(crawls_by_date.items(), key=lambda item: item[0])}\n\n    # return how avg. space between crawls\n    avg_diff, avg_daily_crawls = self._get_avg_space_between_crawls(crawls_by_date, url)\n\n    return crawls_by_date, avg_diff, avg_daily_crawls\n</code></pre>"},{"location":"reference/crawl_log_analyzer/#seotools.logs.CrawlLogAnalyzer.crawl_frequency_by_url","title":"<code>crawl_frequency_by_url(url)</code>","text":"<p>Find the number of times a URL has been crawled.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to analyze.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of times the URL was crawled.</p> Source code in <code>seotools/logs.py</code> <pre><code>def crawl_frequency_by_url(self, url: str) -&gt; int:\n\"\"\"\n    Find the number of times a URL has been crawled.\n\n    Args:\n        url (str): The URL to analyze.\n\n    Returns:\n        int: The number of times the URL was crawled.\n    \"\"\"\n\n    return self.log_file[self.log_file[0] == url].shape[0]\n</code></pre>"},{"location":"reference/crawl_log_analyzer/#seotools.logs.CrawlLogAnalyzer.get_count","title":"<code>get_count(col)</code>","text":"<p>Count the number of times a value appears in a column.</p> <p>Parameters:</p> Name Type Description Default <code>col</code> <code>str</code> <p>The column to analyze.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of values and the number of times they appear in the column.</p> Source code in <code>seotools/logs.py</code> <pre><code>def get_count(self, col: str) -&gt; dict:\n\"\"\"\n    Count the number of times a value appears in a column.\n\n    Args:\n        col (str): The column to analyze.\n\n    Returns:\n        dict: A dictionary of values and the number of times they appear in the column.\n    \"\"\"\n    data =  self.log_file[col].value_counts().to_dict()\n\n    return {k: v for k, v in data.items() if k != '-'}\n</code></pre>"},{"location":"reference/crawl_log_analyzer/#seotools.logs.CrawlLogAnalyzer.get_top_urls","title":"<code>get_top_urls(n=10)</code>","text":"<p>Find the top n most crawled URLs.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The number of URLs to return.</p> <code>10</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of URLs and the number of times they were crawled.</p> Source code in <code>seotools/logs.py</code> <pre><code>def get_top_urls(self, n: int = 10) -&gt; dict:\n\"\"\"\n    Find the top n most crawled URLs.\n\n    Args:\n        n (int): The number of URLs to return.\n\n    Returns:\n        dict: A dictionary of URLs and the number of times they were crawled.\n    \"\"\"\n    return self.log_file['path'].value_counts().head(n).to_dict()\n</code></pre>"},{"location":"reference/crawl_log_analyzer/#seotools.logs.CrawlLogAnalyzer.get_unique","title":"<code>get_unique(col)</code>","text":"<p>Get the unique values in a column.</p> <p>Parameters:</p> Name Type Description Default <code>col</code> <code>str</code> <p>The column to analyze.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of unique values in the column.</p> Example <pre><code>from seotools.logs import CrawLogAnalyzer\n\nanalyzer = CrawlLogAnalyzer(\"access.log\")\n\nanalyzer.get_unique(\"request\")\n</code></pre> Source code in <code>seotools/logs.py</code> <pre><code>def get_unique(self, col: str) -&gt; list:\n\"\"\"\n    Get the unique values in a column.\n\n    Args:\n        col (str): The column to analyze.\n\n    Returns:\n        list: A list of unique values in the column.\n\n    Example:\n        ```python\n        from seotools.logs import CrawLogAnalyzer\n\n        analyzer = CrawlLogAnalyzer(\"access.log\")\n\n        analyzer.get_unique(\"request\")\n        ```\n    \"\"\"\n    return self.log_file[col].unique()\n</code></pre>"}]}